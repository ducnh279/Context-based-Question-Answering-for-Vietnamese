# üåü Introduction to Vietnamese Transformer-based Question Answering
This project presents a fine-tuned implementation of the **XLM-RoBERTa** base model, which is a multilingual transformer-based language model trained on 100 languages from Hugging Face. The model is specifically fine-tuned for Vietnamese language question answering task.

# üí° Model
The XLM-RoBERTa base model, a powerful multilingual transformer-based language model that has demonstrated exceptional performance in various natural language processing tasks, is utilized in the this project. It leverages a self-attention mechanism to capture long-range dependencies and acquire knowledge of complex relationships between words and sentences. The base model offers an optimal trade-off between performance and computational resources when compared to the larger XLM-RoBERTa model

# üìö Training Data Sources
Despite the limited amount of training data, I was able to achieve promising results thanks to the power of the XLM-RoBERTa model. To fine-tune the model, I used a combination of translated datasets of question-answer pairs, as well as a Vietnamese text dataset released by Zalo AI Challenge 2022.

| Dataset | Description | Link |
| --- | --- | --- |
| Vietnamese XSquad | Question-answer pairs dataset released by DeepMind | [Link](https://raw.githubusercontent.com/deepmind/xquad/master/xquad.vi.json) |
| Bert-Vietnamese-Question-Answering | Vietnamese language dataset for question answering | [Link](https://raw.githubusercontent.com/mailong25/bert-vietnamese-question-answering/master/dataset/train-v2.0.json) |
| E2E Question Answering data | Vietnamese text dataset released by Zalo AI Challenge 2022 | [Link](https://www.kaggle.com/datasets/ducnh279/nlp-data) |

# ‚ö°Optimizing Fine-tuning Process
To optimize the fine-tuning process, I used several techniques, including automatic mixed precision, dynamic loss scaling, and parallelized data loading using multiprocessing.

# üìì Notebooks
All the code for this project is available on Kaggle notebooks. You can find the notebooks at:

| Notebook | Link |
| --- | --- |
| Prepare dataset | [Link](https://www.kaggle.com/code/ducnh279/prep-qa-dataset) |
| Set-ups & fine-tuning | [Link](https://www.kaggle.com/code/ducnh279/qa-training) |  

# üìà Results
Here are some examples of how the XLM-RoBERTa model can effectively answer questions in Vietnamese:
| Question | Context | Model Answer |
| --- | --- | --- |
| Elon Musk l√† ng∆∞·ªùi n∆∞·ªõc n√†o? | Elon Reeve Musk FRS (sinh ng√†y 28 th√°ng 6 nƒÉm 1971), l√† m·ªôt k·ªπ s∆∞, nh√† t√†i phi·ªát, nh√† ph√°t minh, doanh nh√¢n c√¥ng ngh·ªá v√† nh√† t·ª´ thi·ªán ng∆∞·ªùi M·ªπ g·ªëc Nam Phi. | ng∆∞·ªùi M·ªπ g·ªëc Nam Phi |
| Elon Musk sinh ng√†y bao nhi√™u? | Elon Reeve Musk FRS (sinh ng√†y 28 th√°ng 6 nƒÉm 1971), l√† m·ªôt k·ªπ s∆∞, nh√† t√†i phi·ªát, nh√† ph√°t minh, doanh nh√¢n c√¥ng ngh·ªá v√† nh√† t·ª´ thi·ªán ng∆∞·ªùi M·ªπ g·ªëc Nam Phi. | ng√†y 28 th√°ng 6 nƒÉm 1971 |
| Em trai Elon t√™n l√† g√¨? | Elon Musk c√πng v·ªõi em trai, Kimbal, ƒë·ªìng s√°ng l·∫≠p ra Zip2, m·ªôt c√¥ng ty ph·∫ßn m·ªÅm web v√† ƒë∆∞·ª£c h√£ng Compaq mua l·∫°i v·ªõi gi√° 340 tri·ªáu USD v√†o nƒÉm 1999. | Kimbal |
| H√£ng Compaq mua l·∫°i Zip2 v·ªõi gi√° bao nhi√™u? | Elon Musk c√πng v·ªõi em trai, Kimbal, ƒë·ªìng s√°ng l·∫≠p ra Zip2, m·ªôt c√¥ng ty ph·∫ßn m·ªÅm web v√† ƒë∆∞·ª£c h√£ng Compaq mua l·∫°i v·ªõi gi√° 340 tri·ªáu USD v√†o nƒÉm 1999. | 340 tri·ªáu USD |

# Conclusion
The aim of this project is to showcase the application of XLM-RoBERTa base model in Vietnamese language question answering. It is my sincere hope that this work may encourage other researchers to delve deeper into the possibilities of transformer-based models and multilingual models for natural language processing tasks.
